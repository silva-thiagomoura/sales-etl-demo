{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946d51c0",
   "metadata": {},
   "source": [
    "# Sales ETL - End-to-End PySpark Demo \n",
    "\n",
    "This notebook demonstrates an end-to-end ETL pipeline using PySpark.\n",
    "\n",
    "Pipeline features:\n",
    "- Generate mock sales data\n",
    "- Extract from CSV\n",
    "- Data quality checks + schema mapping\n",
    "- Hash (encrypt) sensitive IDs\n",
    "- Incremental load using watermark\n",
    "- Write final table into Postgres\n",
    "- Save output into Parquet (partitioned)\n",
    "- Log ETL runs into Postgres table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78af5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 13:09:46 WARN Utils: Your hostname, Thiagos-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.11 instead (on interface en0)\n",
      "26/02/10 13:09:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "26/02/10 13:09:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/10 13:09:47 WARN DependencyUtils: Local jar /Users/thiagomoura/Documents/projetos/sales-etl-pyspark/notebooks/jars/postgresql-42.7.3.jar does not exist, skipping.\n",
      "26/02/10 13:09:47 INFO SparkContext: Running Spark version 3.4.1\n",
      "26/02/10 13:09:47 INFO ResourceUtils: ==============================================================\n",
      "26/02/10 13:09:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "26/02/10 13:09:47 INFO ResourceUtils: ==============================================================\n",
      "26/02/10 13:09:47 INFO SparkContext: Submitted application: sales_etl_spark_demo\n",
      "26/02/10 13:09:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "26/02/10 13:09:47 INFO ResourceProfile: Limiting resource is cpu\n",
      "26/02/10 13:09:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "26/02/10 13:09:47 INFO SecurityManager: Changing view acls to: thiagomoura\n",
      "26/02/10 13:09:47 INFO SecurityManager: Changing modify acls to: thiagomoura\n",
      "26/02/10 13:09:47 INFO SecurityManager: Changing view acls groups to: \n",
      "26/02/10 13:09:47 INFO SecurityManager: Changing modify acls groups to: \n",
      "26/02/10 13:09:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: thiagomoura; groups with view permissions: EMPTY; users with modify permissions: thiagomoura; groups with modify permissions: EMPTY\n",
      "26/02/10 13:09:48 INFO Utils: Successfully started service 'sparkDriver' on port 59485.\n",
      "26/02/10 13:09:48 INFO SparkEnv: Registering MapOutputTracker\n",
      "26/02/10 13:09:48 INFO SparkEnv: Registering BlockManagerMaster\n",
      "26/02/10 13:09:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "26/02/10 13:09:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "26/02/10 13:09:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "26/02/10 13:09:48 INFO DiskBlockManager: Created local directory at /private/var/folders/yj/8f9csmgd5_5d8w9cx2tj11qm0000gn/T/blockmgr-8b8fe3af-7369-45fa-b2cc-baed10560384\n",
      "26/02/10 13:09:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "26/02/10 13:09:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "26/02/10 13:09:49 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "26/02/10 13:09:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/02/10 13:09:49 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "26/02/10 13:09:49 ERROR SparkContext: Failed to add /Users/thiagomoura/Documents/projetos/sales-etl-pyspark/notebooks/jars/postgresql-42.7.3.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/thiagomoura/Documents/projetos/sales-etl-pyspark/notebooks/jars/postgresql-42.7.3.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "26/02/10 13:09:49 INFO Executor: Starting executor ID driver on host 192.168.1.11\n",
      "26/02/10 13:09:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "26/02/10 13:09:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59487.\n",
      "26/02/10 13:09:49 INFO NettyBlockTransferService: Server created on 192.168.1.11:59487\n",
      "26/02/10 13:09:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "26/02/10 13:09:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.11, 59487, None)\n",
      "26/02/10 13:09:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.11:59487 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.1.11, 59487, None)\n",
      "26/02/10 13:09:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.11, 59487, None)\n",
      "26/02/10 13:09:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.11, 59487, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark created with Postgres JDBC driver!\n",
      "spark.jars = /Users/thiagomoura/Documents/projetos/sales-etl-pyspark/notebooks/jars/postgresql-42.7.3.jar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "postgres_jar = os.path.abspath(\"jars/postgresql-42.7.3.jar\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"sales_etl_spark_demo\")\n",
    "    .config(\"spark.jars\", postgres_jar)\n",
    "    .config(\"spark.driver.extraClassPath\", postgres_jar)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark created with Postgres JDBC driver!\")\n",
    "print(\"spark.jars =\", spark.sparkContext.getConf().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85662b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports + Setup -----------------------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754ddf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- CONFIGURATION ---------------------------------\n",
    "\n",
    "\n",
    "CFG = {\n",
    "    \"app\": {\"name\": \"sales_etl_demo\"},\n",
    "    \"paths\": {\n",
    "        \"raw_data_dir\": \"data/raw\",\n",
    "        \"state_dir\": \"data/state\",\n",
    "        \"parquet_output_dir\": \"data/output/sales_parquet\"\n",
    "    },\n",
    "    \"spark\": {\n",
    "        \"master\": \"local[*]\",\n",
    "        \"app_name\": \"sales_etl_spark_demo\",\n",
    "        \"shuffle_partitions\": 4\n",
    "    },\n",
    "    \"db\": {\n",
    "        \"url\": \"jdbc:postgresql://localhost:5432/sales_db\",\n",
    "        \"user\": \"sales_user\",\n",
    "        \"password\": os.getenv(\"DB_PASSWORD\", \"sales_pass\"),  \n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"table\": \"sales\",\n",
    "        \"log_table\": \"etl_run_log\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# State file for watermark\n",
    "WATERMARK_PATH = os.path.join(CFG[\"paths\"][\"state_dir\"], \"watermark.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- SPARK SESSION --------------------------\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(CFG[\"spark\"][\"master\"])\n",
    "    .appName(CFG[\"spark\"][\"app_name\"])\n",
    "    .config(\"spark.sql.shuffle.partitions\", CFG[\"spark\"][\"shuffle_partitions\"])\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- MOCK DATA GENERATOR -------------------------------\n",
    "\n",
    "\n",
    "def generate_mock_sales_csv(output_path: str, n_rows: int):\n",
    "    \"\"\"\n",
    "    Generates a mock sales CSV file.\n",
    "\n",
    "    Why this is useful:\n",
    "    - Allows a full ETL demo without real sensitive data\n",
    "    - Enables repeatable interview execution\n",
    "    - Produces a sequential transaction_id (used as watermark)\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    base_time = datetime.now() - timedelta(days=10)\n",
    "\n",
    "    rows = []\n",
    "    for i in range(n_rows):\n",
    "        transaction_id = i + 1\n",
    "        customer_id = random.randint(1000, 5000)\n",
    "        product_id = random.randint(1, 300)\n",
    "        quantity = random.randint(1, 10)\n",
    "\n",
    "        ts = base_time + timedelta(minutes=random.randint(0, 60 * 24 * 10))\n",
    "\n",
    "        rows.append({\n",
    "            \"transaction_id\": transaction_id,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"product_id\": product_id,\n",
    "            \"quantity\": quantity,\n",
    "            \"timestamp\": ts.isoformat()\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Mock CSV generated: {output_path} ({n_rows} rows)\")\n",
    "\n",
    "\n",
    "RAW_CSV_PATH = os.path.join(CFG[\"paths\"][\"raw_data_dir\"], \"sales1.csv\")\n",
    "\n",
    "# Generate data only if file does not exist (avoid overwriting)\n",
    "if not os.path.exists(RAW_CSV_PATH):\n",
    "    generate_mock_sales_csv(RAW_CSV_PATH, n_rows=500)\n",
    "else:\n",
    "    print(f\"CSV already exists: {RAW_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dc640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ EXTRACT -----------------------------\n",
    "\n",
    "\n",
    "def extract_sales_csv(spark, path: str):\n",
    "    \"\"\"\n",
    "    Reads the raw CSV into a Spark DataFrame.\n",
    "\n",
    "    - header=True: reads column names\n",
    "    - inferSchema=True: Spark infers types (good for demo, not for prod)\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(path)\n",
    "    )\n",
    "\n",
    "    # Select only the expected columns (ignore extra columns if any)\n",
    "    df = df.select(\n",
    "        F.col(\"transaction_id\"),\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"product_id\"),\n",
    "        F.col(\"quantity\"),\n",
    "        F.col(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "raw_df = extract_sales_csv(spark, RAW_CSV_PATH)\n",
    "\n",
    "print(\"Raw rows:\", raw_df.count())\n",
    "raw_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ TRANSFORM (DQ + schema mapping) --------------------------\n",
    "\n",
    "\n",
    "def transform_sales(df):\n",
    "    \"\"\"\n",
    "    Applies:\n",
    "    - schema enforcement\n",
    "    - data quality filters\n",
    "    - outlier filtering\n",
    "    - adds sale_date\n",
    "    \"\"\"\n",
    "\n",
    "    # Schema evolution: ensure required columns exist\n",
    "    required_cols = [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\", \"timestamp\"]\n",
    "\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None).cast(StringType()))\n",
    "\n",
    "    # Casts\n",
    "    df = (\n",
    "        df.withColumn(\"transaction_id\", F.col(\"transaction_id\").cast(IntegerType()))\n",
    "          .withColumn(\"customer_id\", F.col(\"customer_id\").cast(IntegerType()))\n",
    "          .withColumn(\"product_id\", F.col(\"product_id\").cast(IntegerType()))\n",
    "          .withColumn(\"quantity\", F.col(\"quantity\").cast(IntegerType()))\n",
    "          .withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "    )\n",
    "\n",
    "    # Data quality: remove nulls on critical columns\n",
    "    df = df.filter(F.col(\"transaction_id\").isNotNull())\n",
    "    df = df.filter(F.col(\"customer_id\").isNotNull())\n",
    "    df = df.filter(F.col(\"product_id\").isNotNull())\n",
    "    df = df.filter(F.col(\"quantity\").isNotNull())\n",
    "    df = df.filter(F.col(\"timestamp\").isNotNull())\n",
    "\n",
    "    # Outlier handling (business rule)\n",
    "    df = df.filter((F.col(\"quantity\") > 0) & (F.col(\"quantity\") <= 100))\n",
    "\n",
    "    # Derive sale_date (common analytical column)\n",
    "    df = df.withColumn(\"sale_date\", F.to_date(\"timestamp\"))\n",
    "\n",
    "    # Final schema\n",
    "    df = df.select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"quantity\",\n",
    "        \"sale_date\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "clean_df = transform_sales(raw_df)\n",
    "\n",
    "print(\"Clean rows:\", clean_df.count())\n",
    "clean_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- ENCRYPTION (HASH) ------------------------------\n",
    "\n",
    "def encrypt_ids(df):\n",
    "    \"\"\"\n",
    "    Hashes customer_id and product_id using SHA-256.\n",
    "\n",
    "    Why:\n",
    "    - protects sensitive identifiers\n",
    "    - still allows joins and analytics (consistent hash)\n",
    "\n",
    "    Note:\n",
    "    - This is hashing, not reversible encryption.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"customer_id_hash\", F.sha2(F.col(\"customer_id\").cast(\"string\"), 256))\n",
    "          .withColumn(\"product_id_hash\", F.sha2(F.col(\"product_id\").cast(\"string\"), 256))\n",
    "    )\n",
    "\n",
    "\n",
    "# Show before hashing\n",
    "print(\"Before hashing:\")\n",
    "clean_df.select(\"customer_id\", \"product_id\").show(5)\n",
    "\n",
    "hashed_df = encrypt_ids(clean_df)\n",
    "\n",
    "print(\"After hashing:\")\n",
    "hashed_df.select(\"customer_id_hash\", \"product_id_hash\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- WATERMARK (incremental state) -------------------------\n",
    "\n",
    "\n",
    "def read_watermark(path):\n",
    "    \"\"\"\n",
    "    Reads last_transaction_id from watermark JSON file.\n",
    "    Returns None if file doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f).get(\"last_transaction_id\")\n",
    "\n",
    "\n",
    "def write_watermark(path, last_id):\n",
    "    \"\"\"\n",
    "    Writes last_transaction_id to watermark JSON file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"last_transaction_id\": int(last_id)}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- ETL RUN LOGGING INTO POSTGRES --------------------\n",
    "\n",
    "\n",
    "def log_etl_run_to_postgres(\n",
    "    spark,\n",
    "    cfg,\n",
    "    run_id: str,\n",
    "    status: str,\n",
    "    row_count: int,\n",
    "    started_at: str,\n",
    "    finished_at: str,\n",
    "    error_message: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes ETL execution logs into Postgres table.\n",
    "\n",
    "    This is a very strong feature for interviews because it shows:\n",
    "    - observability\n",
    "    - operational maturity\n",
    "    \"\"\"\n",
    "\n",
    "    log_df = spark.createDataFrame([{\n",
    "        \"run_id\": run_id,\n",
    "        \"status\": status,\n",
    "        \"row_count\": int(row_count),\n",
    "        \"started_at\": started_at,\n",
    "        \"finished_at\": finished_at,\n",
    "        \"error_message\": error_message\n",
    "    }])\n",
    "\n",
    "    props = {\n",
    "        \"user\": cfg[\"db\"][\"user\"],\n",
    "        \"password\": cfg[\"db\"][\"password\"],\n",
    "        \"driver\": cfg[\"db\"][\"driver\"]\n",
    "    }\n",
    "\n",
    "    log_df.write.jdbc(\n",
    "        url=cfg[\"db\"][\"url\"],\n",
    "        table=cfg[\"db\"][\"log_table\"],\n",
    "        mode=\"append\",\n",
    "        properties=props\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99923dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- LOAD STEP ------------------- \n",
    "\n",
    "def load_sales_incremental(df, cfg, watermark_path):\n",
    "    \"\"\"\n",
    "    Loads the final dataset incrementally into Postgres.\n",
    "\n",
    "    Incremental logic:\n",
    "    - If watermark exists: load only transaction_id > watermark\n",
    "    - Else: full load (first run)\n",
    "\n",
    "    Also writes the final dataset into partitioned Parquet.\n",
    "    \"\"\"\n",
    "\n",
    "    last_id = read_watermark(watermark_path)\n",
    "\n",
    "    if last_id:\n",
    "        print(f\"Incremental mode enabled. last_transaction_id={last_id}\")\n",
    "        df_to_load = df.filter(F.col(\"transaction_id\") > F.lit(last_id))\n",
    "    else:\n",
    "        print(\"No watermark found. Running full load (first run).\")\n",
    "        df_to_load = df\n",
    "\n",
    "    # If no new rows, exit early\n",
    "    if df_to_load.rdd.isEmpty():\n",
    "        print(\"No new rows to load. Skipping load.\")\n",
    "        return 0\n",
    "\n",
    "    # Hash IDs before loading\n",
    "    df_to_load = encrypt_ids(df_to_load)\n",
    "\n",
    "    # Repartition (demo parallelism)\n",
    "    df_to_load = df_to_load.repartition(4)\n",
    "\n",
    "    props = {\n",
    "        \"user\": cfg[\"db\"][\"user\"],\n",
    "        \"password\": cfg[\"db\"][\"password\"],\n",
    "        \"driver\": cfg[\"db\"][\"driver\"]\n",
    "    }\n",
    "\n",
    "    \n",
    "    # 1) Write to Postgres\n",
    "    \n",
    "    df_to_load.write.jdbc(\n",
    "        url=cfg[\"db\"][\"url\"],\n",
    "        table=cfg[\"db\"][\"table\"],\n",
    "        mode=\"append\",\n",
    "        properties=props\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 2) Write to Parquet (partitioned)\n",
    "    \n",
    "    parquet_path = cfg[\"paths\"][\"parquet_output_dir\"]\n",
    "\n",
    "    (\n",
    "        df_to_load\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(\"sale_date\")  # partition strategy for analytics\n",
    "        .parquet(parquet_path)\n",
    "    )\n",
    "\n",
    "    # Update watermark\n",
    "    max_id = df_to_load.agg(F.max(\"transaction_id\").alias(\"max_id\")).collect()[0][\"max_id\"]\n",
    "    if max_id:\n",
    "        write_watermark(watermark_path, max_id)\n",
    "        print(f\"Watermark updated: last_transaction_id={max_id}\")\n",
    "\n",
    "    return df_to_load.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# RUN ETL PIPELINE\n",
    "# ==============================\n",
    "\n",
    "run_id = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "started_at = datetime.now().isoformat()\n",
    "\n",
    "try:\n",
    "    print(f\"Starting ETL run_id={run_id}\")\n",
    "\n",
    "    # Extract\n",
    "    raw_df = extract_sales_csv(spark, RAW_CSV_PATH)\n",
    "\n",
    "    # Transform\n",
    "    clean_df = transform_sales(raw_df)\n",
    "\n",
    "    # Load (incremental)\n",
    "    loaded_rows = load_sales_incremental(clean_df, CFG, WATERMARK_PATH)\n",
    "\n",
    "    status = \"SUCCESS\"\n",
    "    error_message = None\n",
    "\n",
    "except Exception as e:\n",
    "    status = \"FAILED\"\n",
    "    loaded_rows = 0\n",
    "    error_message = str(e)\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    finished_at = datetime.now().isoformat()\n",
    "\n",
    "    # Log run into Postgres\n",
    "    log_etl_run_to_postgres(\n",
    "        spark=spark,\n",
    "        cfg=CFG,\n",
    "        run_id=run_id,\n",
    "        status=status,\n",
    "        row_count=loaded_rows,\n",
    "        started_at=started_at,\n",
    "        finished_at=finished_at,\n",
    "        error_message=error_message\n",
    "    )\n",
    "\n",
    "    print(f\"ETL finished with status={status}, loaded_rows={loaded_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# READ FINAL TABLE FROM POSTGRES\n",
    "# ==============================\n",
    "\n",
    "props = {\n",
    "    \"user\": CFG[\"db\"][\"user\"],\n",
    "    \"password\": CFG[\"db\"][\"password\"],\n",
    "    \"driver\": CFG[\"db\"][\"driver\"]\n",
    "}\n",
    "\n",
    "sales_db_df = spark.read.jdbc(\n",
    "    url=CFG[\"db\"][\"url\"],\n",
    "    table=CFG[\"db\"][\"table\"],\n",
    "    properties=props\n",
    ")\n",
    "\n",
    "print(\"Rows in Postgres table:\", sales_db_df.count())\n",
    "sales_db_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# READ ETL LOG TABLE\n",
    "# ==============================\n",
    "\n",
    "log_db_df = spark.read.jdbc(\n",
    "    url=CFG[\"db\"][\"url\"],\n",
    "    table=CFG[\"db\"][\"log_table\"],\n",
    "    properties=props\n",
    ")\n",
    "\n",
    "log_db_df.orderBy(F.col(\"started_at\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# READ PARQUET OUTPUT\n",
    "# ==============================\n",
    "\n",
    "parquet_path = CFG[\"paths\"][\"parquet_output_dir\"]\n",
    "\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "print(\"Rows in Parquet:\", parquet_df.count())\n",
    "parquet_df.show(10, truncate=False)\n",
    "\n",
    "print(\"Partitions available (sale_date):\")\n",
    "parquet_df.select(\"sale_date\").distinct().orderBy(\"sale_date\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# SIMPLE ANALYTICS (Spark)\n",
    "# ==============================\n",
    "\n",
    "# Top products by total quantity\n",
    "top_products = (\n",
    "    parquet_df.groupBy(\"product_id_hash\")\n",
    "    .agg(F.sum(\"quantity\").alias(\"total_quantity\"))\n",
    "    .orderBy(F.desc(\"total_quantity\"))\n",
    ")\n",
    "\n",
    "top_products.show(10, truncate=False)\n",
    "\n",
    "# Sales per day\n",
    "sales_per_day = (\n",
    "    parquet_df.groupBy(\"sale_date\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"transactions\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\")\n",
    "    )\n",
    "    .orderBy(\"sale_date\")\n",
    ")\n",
    "\n",
    "sales_per_day.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcc169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Improvements (Production-Ready Ideas)\n",
    "\n",
    "### 1) Config + Validation\n",
    "- Use Pydantic to validate config types\n",
    "- Separate configs per environment (dev/uat/prod)\n",
    "\n",
    "### 2) Better Incremental Strategy\n",
    "- Use timestamp watermark (event time) instead of transaction_id\n",
    "- Support upserts (merge) instead of append-only\n",
    "\n",
    "### 3) Observability\n",
    "- Add metrics (records read, records dropped, invalid rows)\n",
    "- Send logs to a monitoring tool (Datadog, Prometheus, ELK)\n",
    "\n",
    "### 4) Security\n",
    "- Use Secrets Manager / Key Vault\n",
    "- Use proper encryption (KMS) instead of only hashing\n",
    "\n",
    "### 5) Data Lake Layout\n",
    "- Use a layered architecture:\n",
    "  - bronze/raw\n",
    "  - silver/clean\n",
    "  - gold/analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f1a55e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o69.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yj/8f9csmgd5_5d8w9cx2tj11qm0000gn/T/ipykernel_16876/3464642924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m test_df = spark.read.jdbc(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"(select 1 as ok) t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mjpredicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjpredicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o69.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "props = {\n",
    "    \"user\": CFG[\"db\"][\"user\"],\n",
    "    \"password\": CFG[\"db\"][\"password\"],\n",
    "    \"driver\": CFG[\"db\"][\"driver\"]\n",
    "}\n",
    "\n",
    "test_df = spark.read.jdbc(\n",
    "    url=CFG[\"db\"][\"url\"],\n",
    "    table=\"(select 1 as ok) t\",\n",
    "    properties=props\n",
    ")\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21baf552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
