{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946d51c0",
   "metadata": {},
   "source": [
    "# Sales ETL - End-to-End PySpark Demo \n",
    "\n",
    "This notebook demonstrates an end-to-end ETL pipeline using PySpark.\n",
    "\n",
    "Pipeline features:\n",
    "- Generate mock sales data\n",
    "- Extract from CSV\n",
    "- Data quality checks + schema mapping\n",
    "- Hash (encrypt) sensitive IDs\n",
    "- Incremental load using watermark\n",
    "- Write final table into Postgres\n",
    "- Save output into Parquet (partitioned)\n",
    "- Log ETL runs into Postgres table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85662b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Imports + Setup -----------------------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, TimestampType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d776e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--jars /Users/thiagomoura/Documents/projetos/sales-etl-pyspark/jars/postgresql-42.7.3.jar pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1815ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 13:37:35 WARN Utils: Your hostname, Thiagos-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.11 instead (on interface en0)\n",
      "26/02/10 13:37:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "26/02/10 13:37:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/10 13:37:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/02/10 13:37:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754ddf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- CONFIGURATION ---------------------------------\n",
    "\n",
    "\n",
    "CFG = {\n",
    "    \"app\": {\"name\": \"sales_etl_demo\"},\n",
    "    \"paths\": {\n",
    "        \"raw_data_dir\": \"data/raw\",\n",
    "        \"state_dir\": \"data/state\",\n",
    "        \"parquet_output_dir\": \"data/output/sales_parquet\"\n",
    "    },\n",
    "    \"spark\": {\n",
    "        \"master\": \"local[*]\",\n",
    "        \"app_name\": \"sales_etl_spark_demo\",\n",
    "        \"shuffle_partitions\": 4\n",
    "    },\n",
    "    \"db\": {\n",
    "        \"url\": \"jdbc:postgresql://localhost:5432/sales_db\",\n",
    "        \"user\": \"sales_user\",\n",
    "        \"password\": os.getenv(\"DB_PASSWORD\", \"sales_pass\"),  \n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"table\": \"sales\",\n",
    "        \"log_table\": \"etl_run_log\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# State file for watermark\n",
    "WATERMARK_PATH = os.path.join(CFG[\"paths\"][\"state_dir\"], \"watermark.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf7438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark created with Postgres JDBC driver!\n",
      "spark.jars = file:///Users/thiagomoura/Documents/projetos/sales-etl-pyspark/jars/postgresql-42.7.3.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/10 13:37:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- SPARK SESSION --------------------------\n",
    "\n",
    "postgres_jar = os.path.abspath(\"jars/postgresql-42.7.3.jar\")\n",
    "JDBC_DRIVER_PATH = \"/Users/thiago/libs/postgresql-42.6.0.jar\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(CFG[\"spark\"][\"master\"])\n",
    "    .appName(CFG[\"spark\"][\"app_name\"])\n",
    "    .config(\"spark.jars\", JDBC_DRIVER_PATH) \n",
    "    .config(\"spark.sql.shuffle.partitions\", CFG[\"spark\"][\"shuffle_partitions\"])\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark created with Postgres JDBC driver!\")\n",
    "print(\"spark.jars =\", spark.sparkContext.getConf().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8c4a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV already exists: data/raw/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- MOCK DATA GENERATOR -------------------------------\n",
    "\n",
    "\n",
    "def generate_mock_sales_csv(output_path: str, n_rows: int):\n",
    "    \"\"\"\n",
    "    Generates a mock sales CSV file.\n",
    "\n",
    "    Why this is useful:\n",
    "    - Allows a full ETL demo without real sensitive data\n",
    "    - Enables repeatable interview execution\n",
    "    - Produces a sequential transaction_id (used as watermark)\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    base_time = datetime.now() - timedelta(days=10)\n",
    "\n",
    "    rows = []\n",
    "    for i in range(n_rows):\n",
    "        transaction_id = i + 1\n",
    "        customer_id = random.randint(1000, 5000)\n",
    "        product_id = random.randint(1, 300)\n",
    "        quantity = random.randint(1, 10)\n",
    "\n",
    "        ts = base_time + timedelta(minutes=random.randint(0, 60 * 24 * 10))\n",
    "\n",
    "        rows.append({\n",
    "            \"transaction_id\": transaction_id,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"product_id\": product_id,\n",
    "            \"quantity\": quantity,\n",
    "            \"timestamp\": ts.isoformat()\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Mock CSV generated: {output_path} ({n_rows} rows)\")\n",
    "\n",
    "\n",
    "RAW_CSV_PATH = os.path.join(CFG[\"paths\"][\"raw_data_dir\"], \"sales1.csv\")\n",
    "\n",
    "# Generate data only if file does not exist (avoid overwriting)\n",
    "if not os.path.exists(RAW_CSV_PATH):\n",
    "    generate_mock_sales_csv(RAW_CSV_PATH, n_rows=500)\n",
    "else:\n",
    "    print(f\"CSV already exists: {RAW_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1dc640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows: 500\n",
      "+--------------+-----------+----------+--------+--------------------------+\n",
      "|transaction_id|customer_id|product_id|quantity|timestamp                 |\n",
      "+--------------+-----------+----------+--------+--------------------------+\n",
      "|1             |4115       |128       |8       |2026-02-03 10:50:58.620735|\n",
      "|2             |4632       |115       |8       |2026-02-03 06:11:58.620735|\n",
      "|3             |1696       |106       |7       |2026-02-03 22:34:58.620735|\n",
      "|4             |4920       |100       |6       |2026-02-04 19:56:58.620735|\n",
      "|5             |1304       |208       |9       |2026-02-06 06:35:58.620735|\n",
      "|6             |2778       |255       |5       |2026-02-07 19:22:58.620735|\n",
      "|7             |1005       |155       |10      |2026-02-01 17:19:58.620735|\n",
      "|8             |4834       |277       |8       |2026-02-08 04:53:58.620735|\n",
      "|9             |4000       |128       |2       |2026-02-05 16:18:58.620735|\n",
      "|10            |1633       |246       |5       |2026-02-08 13:57:58.620735|\n",
      "+--------------+-----------+----------+--------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------ EXTRACT -----------------------------\n",
    "\n",
    "\n",
    "def extract_sales_csv(spark, path: str):\n",
    "    \"\"\"\n",
    "    Reads the raw CSV into a Spark DataFrame.\n",
    "\n",
    "    - header=True: reads column names\n",
    "    - inferSchema=True: Spark infers types (good for demo, not for prod)\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(path)\n",
    "    )\n",
    "\n",
    "    # Select only the expected columns (ignore extra columns if any)\n",
    "    df = df.select(\n",
    "        F.col(\"transaction_id\"),\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"product_id\"),\n",
    "        F.col(\"quantity\"),\n",
    "        F.col(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "raw_df = extract_sales_csv(spark, RAW_CSV_PATH)\n",
    "\n",
    "print(\"Raw rows:\", raw_df.count())\n",
    "raw_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b16cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean rows: 500\n",
      "+--------------+-----------+----------+--------+----------+\n",
      "|transaction_id|customer_id|product_id|quantity|sale_date |\n",
      "+--------------+-----------+----------+--------+----------+\n",
      "|1             |4115       |128       |8       |2026-02-03|\n",
      "|2             |4632       |115       |8       |2026-02-03|\n",
      "|3             |1696       |106       |7       |2026-02-03|\n",
      "|4             |4920       |100       |6       |2026-02-04|\n",
      "|5             |1304       |208       |9       |2026-02-06|\n",
      "|6             |2778       |255       |5       |2026-02-07|\n",
      "|7             |1005       |155       |10      |2026-02-01|\n",
      "|8             |4834       |277       |8       |2026-02-08|\n",
      "|9             |4000       |128       |2       |2026-02-05|\n",
      "|10            |1633       |246       |5       |2026-02-08|\n",
      "+--------------+-----------+----------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ TRANSFORM (DQ + schema mapping) --------------------------\n",
    "\n",
    "\n",
    "def transform_sales(df):\n",
    "    \"\"\"\n",
    "    Applies:\n",
    "    - schema enforcement\n",
    "    - data quality filters\n",
    "    - outlier filtering\n",
    "    - adds sale_date\n",
    "    \"\"\"\n",
    "\n",
    "    # Schema evolution: ensure required columns exist\n",
    "    required_cols = [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\", \"timestamp\"]\n",
    "\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None).cast(StringType()))\n",
    "\n",
    "    # Casts\n",
    "    df = (\n",
    "        df.withColumn(\"transaction_id\", F.col(\"transaction_id\").cast(IntegerType()))\n",
    "          .withColumn(\"customer_id\", F.col(\"customer_id\").cast(IntegerType()))\n",
    "          .withColumn(\"product_id\", F.col(\"product_id\").cast(IntegerType()))\n",
    "          .withColumn(\"quantity\", F.col(\"quantity\").cast(IntegerType()))\n",
    "          .withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "    )\n",
    "\n",
    "    # Data quality: remove nulls on critical columns\n",
    "    df = df.filter(F.col(\"transaction_id\").isNotNull())\n",
    "    df = df.filter(F.col(\"customer_id\").isNotNull())\n",
    "    df = df.filter(F.col(\"product_id\").isNotNull())\n",
    "    df = df.filter(F.col(\"quantity\").isNotNull())\n",
    "    df = df.filter(F.col(\"timestamp\").isNotNull())\n",
    "\n",
    "    # Outlier handling (business rule)\n",
    "    df = df.filter((F.col(\"quantity\") > 0) & (F.col(\"quantity\") <= 100))\n",
    "\n",
    "    # Derive sale_date (common analytical column)\n",
    "    df = df.withColumn(\"sale_date\", F.to_date(\"timestamp\"))\n",
    "\n",
    "    # Final schema\n",
    "    df = df.select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"quantity\",\n",
    "        \"sale_date\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "clean_df = transform_sales(raw_df)\n",
    "\n",
    "print(\"Clean rows:\", clean_df.count())\n",
    "clean_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4a6ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before hashing:\n",
      "+-----------+----------+\n",
      "|customer_id|product_id|\n",
      "+-----------+----------+\n",
      "|       4115|       128|\n",
      "|       4632|       115|\n",
      "|       1696|       106|\n",
      "|       4920|       100|\n",
      "|       1304|       208|\n",
      "+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "After hashing:\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|customer_id_hash                                                |product_id_hash                                                 |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|481d60bc802f51580e81af5dc1c0534d6eb255fcbb82bdaf646e8549b7cce4f3|2747b7c718564ba5f066f0523b03e17f6a496b06851333d2d59ab6d863225848|\n",
      "|9509368b9a5172b8d96f18644356e636b4999607ec09c62b6d92d365169cedfa|28dae7c8bde2f3ca608f86d0e16a214dee74c74bee011cdfdd46bc04b655bc14|\n",
      "|e64474fd91f16a0891fb1de23dff06e8bd0a0ee495f1add2ec08a07f1fb63bb4|482d9673cfee5de391f97fde4d1c84f9f8d6f2cf0784fcffb958b4032de7236c|\n",
      "|4dfc32fc79b06f35f05d8b431cdc32ce5b38d1c6cef268fc4d6e83574411b553|ad57366865126e55649ecb23ae1d48887544976efea46a48eb5d85a6eeb4d306|\n",
      "|8ecb5bcd8cd84cc3ffc6f5dc3076d81c0a457a6bd4b305a33f318b623d701c2e|8df66f64b57424391d363fd6b811fed3c430c77597da265025728bd637bad804|\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- ENCRYPTION (HASH) ------------------------------\n",
    "\n",
    "def encrypt_ids(df):\n",
    "    \"\"\"\n",
    "    Hashes customer_id and product_id using SHA-256.\n",
    "\n",
    "    Why:\n",
    "    - protects sensitive identifiers\n",
    "    - still allows joins and analytics (consistent hash)\n",
    "\n",
    "    Note:\n",
    "    - This is hashing, not reversible encryption.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"customer_id_hash\", F.sha2(F.col(\"customer_id\").cast(\"string\"), 256))\n",
    "          .withColumn(\"product_id_hash\", F.sha2(F.col(\"product_id\").cast(\"string\"), 256))\n",
    "    )\n",
    "\n",
    "\n",
    "# Show before hashing\n",
    "print(\"Before hashing:\")\n",
    "clean_df.select(\"customer_id\", \"product_id\").show(5)\n",
    "\n",
    "hashed_df = encrypt_ids(clean_df)\n",
    "\n",
    "print(\"After hashing:\")\n",
    "hashed_df.select(\"customer_id_hash\", \"product_id_hash\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- WATERMARK (incremental state) -------------------------\n",
    "\n",
    "\n",
    "def read_watermark(path):\n",
    "    \"\"\"\n",
    "    Reads last_transaction_id from watermark JSON file.\n",
    "    Returns None if file doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f).get(\"last_transaction_id\")\n",
    "\n",
    "\n",
    "def write_watermark(path, last_id):\n",
    "    \"\"\"\n",
    "    Writes last_transaction_id to watermark JSON file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"last_transaction_id\": int(last_id)}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5e27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- ETL RUN LOGGING INTO POSTGRES --------------------\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, TimestampType\n",
    ")\n",
    "\n",
    "def log_etl_run_to_postgres(\n",
    "    spark,\n",
    "    cfg,\n",
    "    run_id: str,\n",
    "    status: str,\n",
    "    row_count: int,\n",
    "    started_at,\n",
    "    finished_at,\n",
    "    error_message: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs one ETL execution into Postgres with a robust schema.\n",
    "\n",
    "    started_at / finished_at must be datetime.datetime objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert from string to datetime if needed\n",
    "    if isinstance(started_at, str):\n",
    "        started_at = datetime.fromisoformat(started_at)\n",
    "    if isinstance(finished_at, str):\n",
    "        finished_at = datetime.fromisoformat(finished_at)\n",
    "\n",
    "    # Explicit schema\n",
    "    log_schema = StructType([\n",
    "        StructField(\"run_id\", StringType(), False),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"row_count\", IntegerType(), True),\n",
    "        StructField(\"started_at\", TimestampType(), True),\n",
    "        StructField(\"finished_at\", TimestampType(), True),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # Ensure row_count is int\n",
    "    row_count = int(row_count or 0)\n",
    "\n",
    "    # Create DataFrame\n",
    "    log_df = spark.createDataFrame([{\n",
    "        \"run_id\": run_id,\n",
    "        \"status\": status,\n",
    "        \"row_count\": row_count,\n",
    "        \"started_at\": started_at,\n",
    "        \"finished_at\": finished_at,\n",
    "        \"error_message\": error_message\n",
    "    }], schema=log_schema)\n",
    "\n",
    "    # JDBC properties\n",
    "    props = {\n",
    "        \"user\": cfg[\"db\"][\"user\"],\n",
    "        \"password\": cfg[\"db\"][\"password\"],\n",
    "        \"driver\": cfg[\"db\"][\"driver\"]\n",
    "    }\n",
    "\n",
    "    # Write to Postgres\n",
    "    log_df.write.jdbc(\n",
    "        url=cfg[\"db\"][\"url\"],\n",
    "        table=\"etl_run_log\",\n",
    "        mode=\"append\",\n",
    "        properties=props\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99923dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- LOAD STEP ------------------- \n",
    "\n",
    "def load_sales_incremental(df, cfg, watermark_path):\n",
    "    \"\"\"\n",
    "    Loads the final dataset incrementally into Postgres.\n",
    "\n",
    "    Incremental logic:\n",
    "    - If watermark exists: load only transaction_id > watermark\n",
    "    - Else: full load (first run)\n",
    "\n",
    "    Also writes the final dataset into partitioned Parquet.\n",
    "    \"\"\"\n",
    "\n",
    "    last_id = read_watermark(watermark_path)\n",
    "\n",
    "    if last_id:\n",
    "        print(f\"Incremental mode enabled. last_transaction_id={last_id}\")\n",
    "        df_to_load = df.filter(F.col(\"transaction_id\") > F.lit(last_id))\n",
    "    else:\n",
    "        print(\"No watermark found. Running full load (first run).\")\n",
    "        df_to_load = df\n",
    "\n",
    "    # If no new rows, exit early\n",
    "    if df_to_load.rdd.isEmpty():\n",
    "        print(\"No new rows to load. Skipping load.\")\n",
    "        return 0\n",
    "\n",
    "    # Hash IDs before loading\n",
    "    df_to_load = encrypt_ids(df_to_load)\n",
    "\n",
    "    # Repartition (demo parallelism)\n",
    "    df_to_load = df_to_load.repartition(4)\n",
    "\n",
    "    props = {\n",
    "        \"user\": cfg[\"db\"][\"user\"],\n",
    "        \"password\": cfg[\"db\"][\"password\"],\n",
    "        \"driver\": cfg[\"db\"][\"driver\"]\n",
    "    }\n",
    "\n",
    "    \n",
    "    # 1) Write to Postgres\n",
    "    \n",
    "    df_to_load.write.jdbc(\n",
    "        url=cfg[\"db\"][\"url\"],\n",
    "        table=cfg[\"db\"][\"table\"],\n",
    "        mode=\"append\",\n",
    "        properties=props\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 2) Write to Parquet (partitioned)\n",
    "    \n",
    "    parquet_path = cfg[\"paths\"][\"parquet_output_dir\"]\n",
    "\n",
    "    (\n",
    "        df_to_load\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(\"sale_date\")  # partition strategy for analytics\n",
    "        .parquet(parquet_path)\n",
    "    )\n",
    "\n",
    "    # Update watermark\n",
    "    max_id = df_to_load.agg(F.max(\"transaction_id\").alias(\"max_id\")).collect()[0][\"max_id\"]\n",
    "    if max_id:\n",
    "        write_watermark(watermark_path, max_id)\n",
    "        print(f\"Watermark updated: last_transaction_id={max_id}\")\n",
    "\n",
    "    return df_to_load.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f343386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ETL run_id=run_20260210_133756\n",
      "Incremental mode enabled. last_transaction_id=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new rows to load. Skipping load.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL finished with status=SUCCESS, loaded_rows=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# RUN ETL PIPELINE\n",
    "# ==============================\n",
    "\n",
    "run_id = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "started_at = datetime.now().isoformat()\n",
    "\n",
    "try:\n",
    "    print(f\"Starting ETL run_id={run_id}\")\n",
    "\n",
    "    # Extract\n",
    "    raw_df = extract_sales_csv(spark, RAW_CSV_PATH)\n",
    "\n",
    "    # Transform\n",
    "    clean_df = transform_sales(raw_df)\n",
    "\n",
    "    # Load (incremental)\n",
    "    loaded_rows = load_sales_incremental(clean_df, CFG, WATERMARK_PATH)\n",
    "\n",
    "    status = \"SUCCESS\"\n",
    "    error_message = None\n",
    "\n",
    "except Exception as e:\n",
    "    status = \"FAILED\"\n",
    "    loaded_rows = 0\n",
    "    error_message = str(e)\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    finished_at = datetime.now().isoformat()\n",
    "\n",
    "    # Log run into Postgres\n",
    "    log_etl_run_to_postgres(\n",
    "        spark=spark,\n",
    "        cfg=CFG,\n",
    "        run_id=run_id,\n",
    "        status=status,\n",
    "        row_count=loaded_rows,\n",
    "        started_at=started_at,\n",
    "        finished_at=finished_at,\n",
    "        error_message=error_message\n",
    "    )\n",
    "\n",
    "    print(f\"ETL finished with status={status}, loaded_rows={loaded_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e6dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in Postgres table: 50000\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "|transaction_id|customer_id                                                     |product_id                                                      |quantity|sale_date |\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "|41696         |75ab3e0a8f99144f9c4420698980b6f76f85996b520937c7043a29e18527bfef|a68b412c4282555f15546cf6e1fc42893b7e07f271557ceb021821098dd66c1b|4       |2026-01-28|\n",
      "|932           |96382608813353bee4eeaf0635a3b3356276fece94fe8c9cf048871078f8fd14|44cb730c420480a0477b505ae68af508fb90f96cf0ec54c6ad16949dd427f13a|6       |2026-02-02|\n",
      "|1592          |15b9e0db83ca5103d0d81f272584932103478a4850cb788ee4fff20b7ab5c5ba|768b84ef05f655d57fe22d488451f075365f6cd18a13073466aa826cc0ebdbfb|9       |2026-02-03|\n",
      "|36766         |66dfd8bb9bcf776b908c210fc6278225d153e6af94aa7e2d6177f96c5594ba6a|37c20f19f3272b5ccc3a5d80587eb9deb3f4afcf568c4280fb195568da8eb1a2|5       |2026-02-02|\n",
      "|43572         |5119e090c80757fec3c9f1dca46e3481688fed2fea905db0af7994857abb92a6|b4bbe448fde336bb6a7d7d765f36d3327c772b845e7b54c8282aa08c9775ddd7|10      |2026-02-05|\n",
      "|23130         |25e16cbd45433fc6878ceb8f4337dd5a79111fcc13aa7fecdb4525607f63171d|27d719c754aacd492a6dc8a1b76619355abcf5ef473cbec02018d3c57ebbf0d5|6       |2026-02-03|\n",
      "|15167         |cca40327e9be88bde3e98b71aa9f73ce6b44b211460cd920b2869e82118c8510|3e1e967e9b793e908f8eae83c74dba9bcccce6a5535b4b462bd9994537bfe15c|3       |2026-02-05|\n",
      "|40800         |0d46ebf59e025c235530010e020e27215938fc186b51126f2ba2aa6ff99be7db|43974ed74066b207c30ffd0fed5146762e6c60745ac977004bc14507c7c42b50|6       |2026-02-04|\n",
      "|16307         |8c6c42f379f08f03b79653a3230abd5e8079999435030fd8ca703ae35fe9b37a|c0509a487a18b003ba05e505419ebb63e57a29158073e381f57160b5c5b86426|2       |2026-02-02|\n",
      "|23157         |82887006d04d939ffca870bc268a940df6cf01dbdf12e228ccf476d07d7c9424|29db0c6782dbd5000559ef4d9e953e300e2b479eed26d887ef3f92b921c06a67|8       |2026-02-05|\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# READ FINAL TABLE FROM POSTGRES\n",
    "# ==============================\n",
    "\n",
    "props = {\n",
    "    \"user\": CFG[\"db\"][\"user\"],\n",
    "    \"password\": CFG[\"db\"][\"password\"],\n",
    "    \"driver\": CFG[\"db\"][\"driver\"]\n",
    "}\n",
    "\n",
    "sales_db_df = spark.read.jdbc(\n",
    "    url=CFG[\"db\"][\"url\"],\n",
    "    table=CFG[\"db\"][\"table\"],\n",
    "    properties=props\n",
    ")\n",
    "\n",
    "print(\"Rows in Postgres table:\", sales_db_df.count())\n",
    "sales_db_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0b4ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+-------------------------+--------------------------+-------------+\n",
      "|run_id             |status |row_count|started_at               |finished_at               |error_message|\n",
      "+-------------------+-------+---------+-------------------------+--------------------------+-------------+\n",
      "|run_20260210_133756|SUCCESS|0        |2026-02-10 13:37:56.66402|2026-02-10 13:38:00.999985|null         |\n",
      "+-------------------+-------+---------+-------------------------+--------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# READ ETL LOG TABLE\n",
    "# ==============================\n",
    "\n",
    "log_db_df = spark.read.jdbc(\n",
    "    url=CFG[\"db\"][\"url\"],\n",
    "    table=CFG[\"db\"][\"log_table\"],\n",
    "    properties=props\n",
    ")\n",
    "\n",
    "log_db_df.orderBy(F.col(\"started_at\").desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "983c25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet successfully saved at: data/output/sales_parquet\n",
      "Number of rows in Parquet: 50000\n",
      "Sample data:\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "|transaction_id|customer_id                                                     |product_id                                                      |quantity|sale_date |\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "|41696         |75ab3e0a8f99144f9c4420698980b6f76f85996b520937c7043a29e18527bfef|a68b412c4282555f15546cf6e1fc42893b7e07f271557ceb021821098dd66c1b|4       |2026-01-28|\n",
      "|20611         |0a6b81782b5c6d04236250f24ed2b3a27c3afd1a80371ea238ba029cbe5aca0d|4523540f1504cd17100c4835e85b7eefd49911580f8efff0599a8f283be6b9e3|10      |2026-01-28|\n",
      "|41881         |591d48eb061e4e3e4ca2b55451e2353c3922ff23264f22e05f09a9e63b780e2c|80c3cd40fa35f9088b8741bd8be6153de05f661cfeeb4625ffbf5f4a6c3c02c4|1       |2026-01-28|\n",
      "|19413         |16a641c270b45a22f930546aa642a1f8a052790f50c4a7aa08e82b20ebb9ff22|39fa9ec190eee7b6f4dff1100d6343e10918d044c75eac8f9e9a2596173f80c9|10      |2026-01-28|\n",
      "|19590         |aff4b5855447e95d8af83a24048c3f3e2320be9f5dde915ba81015e18802e0b2|a21855da08cb102d1d217c53dc5824a3a795c1c1a44e971bf01ab9da3a2acbbf|9       |2026-01-28|\n",
      "|19387         |caaafd2b3354748b9ba47653f0b2aa084199f23be24cb1fada4dc281f70b2590|a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2389f6d02932c47f8a6|4       |2026-01-28|\n",
      "|34607         |02eb0fab1eaae6e083cf98bf4df565ef80b8a4655af5c772cecb356c447521b4|8bcbb4c131df56f7c79066016241cc4bdf4e58db55c4f674e88b22365bd2e2ad|7       |2026-01-28|\n",
      "|31898         |0edcd7f37d75330b1d55b3353fecdd73dcb60796669ba7f1a45c0985874d883f|e7866fdc6672f827c76f6124ca3eeaff44aff8b7caf4ee1469b2ab887e7e7875|4       |2026-01-28|\n",
      "|24162         |a89d6642c31c33525be583f4333b37d260d14903319a846e4f1e9e091a978592|7b69759630f869f2723875f873935fed29d2d12b10ef763c1c33b8e0004cb405|6       |2026-01-28|\n",
      "|10381         |33f849de3622f07a54fd0011c664a29264a30f2bf9a70fad7857b7ffa5f15e95|bba58959c32abe688d9cb5222b97de973002a67c412d6a8c8d2a79ac692f32b7|6       |2026-01-28|\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "|transaction_id|customer_id                                                     |product_id                                                      |quantity|sale_date |\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "|41696         |75ab3e0a8f99144f9c4420698980b6f76f85996b520937c7043a29e18527bfef|a68b412c4282555f15546cf6e1fc42893b7e07f271557ceb021821098dd66c1b|4       |2026-01-28|\n",
      "|20611         |0a6b81782b5c6d04236250f24ed2b3a27c3afd1a80371ea238ba029cbe5aca0d|4523540f1504cd17100c4835e85b7eefd49911580f8efff0599a8f283be6b9e3|10      |2026-01-28|\n",
      "|41881         |591d48eb061e4e3e4ca2b55451e2353c3922ff23264f22e05f09a9e63b780e2c|80c3cd40fa35f9088b8741bd8be6153de05f661cfeeb4625ffbf5f4a6c3c02c4|1       |2026-01-28|\n",
      "|19413         |16a641c270b45a22f930546aa642a1f8a052790f50c4a7aa08e82b20ebb9ff22|39fa9ec190eee7b6f4dff1100d6343e10918d044c75eac8f9e9a2596173f80c9|10      |2026-01-28|\n",
      "|19590         |aff4b5855447e95d8af83a24048c3f3e2320be9f5dde915ba81015e18802e0b2|a21855da08cb102d1d217c53dc5824a3a795c1c1a44e971bf01ab9da3a2acbbf|9       |2026-01-28|\n",
      "|19387         |caaafd2b3354748b9ba47653f0b2aa084199f23be24cb1fada4dc281f70b2590|a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2389f6d02932c47f8a6|4       |2026-01-28|\n",
      "|34607         |02eb0fab1eaae6e083cf98bf4df565ef80b8a4655af5c772cecb356c447521b4|8bcbb4c131df56f7c79066016241cc4bdf4e58db55c4f674e88b22365bd2e2ad|7       |2026-01-28|\n",
      "|31898         |0edcd7f37d75330b1d55b3353fecdd73dcb60796669ba7f1a45c0985874d883f|e7866fdc6672f827c76f6124ca3eeaff44aff8b7caf4ee1469b2ab887e7e7875|4       |2026-01-28|\n",
      "|24162         |a89d6642c31c33525be583f4333b37d260d14903319a846e4f1e9e091a978592|7b69759630f869f2723875f873935fed29d2d12b10ef763c1c33b8e0004cb405|6       |2026-01-28|\n",
      "|10381         |33f849de3622f07a54fd0011c664a29264a30f2bf9a70fad7857b7ffa5f15e95|bba58959c32abe688d9cb5222b97de973002a67c412d6a8c8d2a79ac692f32b7|6       |2026-01-28|\n",
      "+--------------+----------------------------------------------------------------+----------------------------------------------------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE FINAL DF TO PARQUET (PARTITIONED)\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Output path for Parquet (from CFG)\n",
    "parquet_path = CFG[\"paths\"].get(\"parquet_output_dir\", \"data/output/sales_parquet\")\n",
    "parquet_path_log = CFG[\"paths\"].get(\"parquet_output_dir\", \"data/output/sales_parquet_log\")\n",
    "\n",
    "# Create folder if it does not exist\n",
    "os.makedirs(parquet_path, exist_ok=True)\n",
    "\n",
    "# Save the final DataFrame (final_df) as Parquet partitioned by sale_date\n",
    "# Overwrite mode to ensure it replaces existing files (partitionBy(\"sale_date\"))\n",
    "log_db_df.write.mode(\"overwrite\").parquet(parquet_path_log)\n",
    "sales_db_df.write.mode(\"overwrite\").partitionBy(\"sale_date\").parquet(parquet_path)\n",
    "\n",
    "print(f\"Parquet successfully saved at: {parquet_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# READ SAVED PARQUET\n",
    "# ==========================================\n",
    "\n",
    "# Read the Parquet back for verification\n",
    "parquet_df_log = spark.read.parquet(parquet_path_log)\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "print(\"Number of rows in Parquet:\", parquet_df.count())\n",
    "print(\"Sample data:\")\n",
    "parquet_df.show(10, truncate=False)\n",
    "\n",
    "parquet_df_log.show(10, truncate=False)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# FUTURE IMPROVEMENT\n",
    "# ==========================================\n",
    "# 1) Save detailed ETL logs (run_id, status, processed row count) in a separate table.\n",
    "# 2) Enable Parquet compression (e.g., parquet(..., compression='snappy')) to save space.\n",
    "# 3) Add incremental control when writing Parquet (append) if ETL runs multiple times.\n",
    "# 4) Validate schema when reading Parquet: spark.read.schema(schema).parquet(path)\n",
    "# 5) Consider partitioning by multiple columns for more efficient queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baef8deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+--------------+\n",
      "|product_id                                                      |total_quantity|\n",
      "+----------------------------------------------------------------+--------------+\n",
      "|16badfc6202cb3f8889e0f2779b19218af4cbb736e56acadce8148aba9a7a9f8|1202          |\n",
      "|d029fa3a95e174a19934857f535eb9427d967218a36ea014b70ad704bc6c8d1c|1142          |\n",
      "|011af72a910ac4acf367eef9e6b761e0980842c30d4e9809840f4141d5163ede|1139          |\n",
      "|f6e0a1e2ac41945a9aa7ff8a8aaa0cebc12a3bcc981a929ad5cf810a090e11ae|1101          |\n",
      "|f8809aff4d69bece79dabe35be0c708b890d7eafb841f121330667b77d2e2590|1098          |\n",
      "|37c20f19f3272b5ccc3a5d80587eb9deb3f4afcf568c4280fb195568da8eb1a2|1088          |\n",
      "|61a229bae1e90331edd986b6bbbe617f7035de88a5bf7c018c3add6c762a6e8d|1083          |\n",
      "|41e521adf8ae7a0f419ee06e1d9fb794162369237b46f64bf5b2b9969b0bcd2e|1074          |\n",
      "|2858dcd1057d3eae7f7d5f782167e24b61153c01551450a628cee722509f6529|1072          |\n",
      "|09895de0407bcb0386733daa14bdb5dfa544505530c634334a05a60f161b71fc|1070          |\n",
      "+----------------------------------------------------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 69:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+\n",
      "|sale_date |transactions|total_quantity|\n",
      "+----------+------------+--------------+\n",
      "|2026-01-27|2648        |14569         |\n",
      "|2026-01-28|5192        |28752         |\n",
      "|2026-01-29|5024        |27680         |\n",
      "|2026-01-30|4900        |27007         |\n",
      "|2026-01-31|5074        |27344         |\n",
      "|2026-02-01|5015        |27438         |\n",
      "|2026-02-02|4989        |27177         |\n",
      "|2026-02-03|4886        |26964         |\n",
      "|2026-02-04|4856        |26571         |\n",
      "|2026-02-05|4996        |27525         |\n",
      "|2026-02-06|2420        |13340         |\n",
      "+----------+------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# SIMPLE ANALYTICS (Spark)\n",
    "# ==============================\n",
    "\n",
    "# Top products by total quantity\n",
    "top_products = (\n",
    "    parquet_df.groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"quantity\").alias(\"total_quantity\"))\n",
    "    .orderBy(F.desc(\"total_quantity\"))\n",
    ")\n",
    "\n",
    "top_products.show(10, truncate=False)\n",
    "\n",
    "# Sales per day\n",
    "sales_per_day = (\n",
    "    parquet_df.groupBy(\"sale_date\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"transactions\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\")\n",
    "    )\n",
    "    .orderBy(\"sale_date\")\n",
    ")\n",
    "\n",
    "sales_per_day.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f4781",
   "metadata": {},
   "source": [
    "# Future Improvements (Production-Ready Ideas)\n",
    "\n",
    "### 1) Config + Validation\n",
    "- Use Pydantic to validate config types\n",
    "- Separate configs per environment (dev/uat/prod)\n",
    "\n",
    "### 2) Better Incremental Strategy\n",
    "- Use timestamp watermark (event time) instead of transaction_id\n",
    "- Support upserts (merge) instead of append-only\n",
    "\n",
    "### 3) Observability\n",
    "- Add metrics (records read, records dropped, invalid rows)\n",
    "- Send logs to a monitoring tool (Datadog, Prometheus, ELK)\n",
    "\n",
    "### 4) Security\n",
    "- Use Secrets Manager / Key Vault\n",
    "- Use proper encryption (KMS) instead of only hashing\n",
    "\n",
    "### 5) Data Lake Layout\n",
    "- Use a layered architecture:\n",
    "  - bronze/raw\n",
    "  - silver/clean\n",
    "  - gold/analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {\n",
    "    \"user\": CFG[\"db\"][\"user\"],\n",
    "    \"password\": CFG[\"db\"][\"password\"],\n",
    "    \"driver\": CFG[\"db\"][\"driver\"]\n",
    "}\n",
    "\n",
    "test_df = spark.read.jdbc(\n",
    "    url=CFG[\"db\"][\"url\"],\n",
    "    table=\"(select 1 as ok) t\",\n",
    "    properties=props\n",
    ")\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21baf552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
